Talk to Transformer

#  Talk to Transformer

See how a modern neural network completes your text. Type a custom snippet or try one of the examples. [Learn more](https://talktotransformer.com/#) below.

[**Follow **@AdamDanielKing**](https://twitter.com/intent/follow?original_referer=https%3A%2F%2Ftalktotransformer.com%2F&ref_src=twsrc%5Etfw&region=follow_link&screen_name=AdamDanielKing&tw_p=followbutton)

for more neat neural networks.
**Update Nov 5: The full-sized GPT-2 is finally released! Try it out. â†“**
Custom prompt
*arrow_drop_down*

##### About

Built by [Adam King](https://adamdking.com/) ([@AdamDanielKing](https://twitter.com/adamdanielking)) as an easier way to play with OpenAI's new machine learning model. In February, OpenAI unveiled a [language model called GPT-2](https://openai.com/blog/better-language-models/) that generates coherent paragraphs of text one word at a time.

This site runs the **full-sized GPT-2** model, called 1558M. Before November 1, OpenAI had only released three smaller, less coherent versions of the model.

While GPT-2 was only trained to predict the next word in a text, it surprisingly learned basic competence in some tasks like translating between languages and answering questions. That's without ever being told that it would be evaluated on those tasks. To learn more, read [OpenAI's blog post](https://openai.com/blog/better-language-models/) or [follow me on Twitter](https://twitter.com/adamdanielking).

###### Acknowledgments

Thanks to [Hugging Face](https://huggingface.co/) for their PyTorch implementation of GPT-2 which I modified to handle batching queries of mixed lengths.