How Google treats Meredith Whittaker is important to potential AI whistleblowers

In a co-written letter, Meredith Whittaker — one of two Google employees who claim they’ve faced retaliation for organizing protests against their employer’s treatment of sexual harassment — alleged that she’d been pressured by Google to “abandon her work” at the [AI Now Institute](https://ainowinstitute.org/), which she’d helped to found, and that she was informed her role would be “changed dramatically.” The situation raises the question of what protections there are for people who speak out concerning AI ethics.

Earlier this month, Whittaker coauthored a [report](https://ainowinstitute.org/discriminatingsystems.pdf) highlighting the growing diversity crisis in the AI sector. “Given decades of concern and investment to redress this imbalance, the current state of the field is alarming,” Whittaker and her coauthors wrote for New York University’s AI Now Institute. “The AI industry needs to acknowledge the gravity of its diversity problem, and admit that existing methods have failed to contend with the uneven distribution of power, and the means by which AI can reinforce such inequality.”

Recommended Videos

New AI technology able to 'read' ones facial impressions

New tech developed by a US-based AI company can scan your face and read your emotions.Affectiva has developed emotional detection technology that can be used to translate human facial expressions. This technology uses two main techniques: computer vision and supervised learning, The Guardian reports.Computer vision is used to clearly identify facial expressions while supervised learning is used to train the algorithm to recognize expressions it has already seen.The company has already gathered data from 7.5 million faces in 87 countries, according to Affectiva's website.The company uses EMFACS or Emotion Facial Action Coding System to recognize facial expressions such as a scowling face for anger or a smiling face for happiness.The Guardian quotes Meredith Whittaker, from the research institute AI Now, as saying that recruiting companies and schools are already using this technique to assess facial expressions of candidates or students.She added that the technology could harm an individual if the analysis isn't completely accurate.RUNDOWN SHOWS:1. AI emotional detection technology2. Showing computer vision and supervised learning3. The Emotional Facial Action Coding System4. Recruiting company icon, school icon, AI technology reading a faceVOICEOVER (in English): "Affectiva has developed emotional detection technology that can be used to translate human facial expressions.""The Guardian reports this technology uses two main techniques: computer vision and supervised learning.""Computer vision is used to clearly identify facial expressions while supervised learning is used to train the algorithm to recognize expressions it has already seen.""The company uses EMFACS or Emotion Facial Action Coding System to recognize facial expressions such as a scowling face for anger or a smiling face for happiness.""The Guardian quotes Meredith Whittaker, from the research institute AI Now, as saying that recruiting companies and schools are already using this technique to assess facial expressions of candidates or students.""Adding that the technology could harm an individual if the analysis isn't completely accurate."SOURCES: The Guardian, MIT Technology Review, Affectiva website, https://www.theguardian.com/technology/2019/mar/06/facial-recognition-software-emotional-sciencehttps://www.technologyreview.com/s/613019/when-our-devices-can-read-our-emotions-affectivas-gabi-zijderveld/https://www.affectiva.com/

Volume 0%

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-buffer js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='159'%3e%3cpath d='M120%2c186.667a66.667%2c66.667%2c0%2c0%2c1%2c0-133.333V40a80%2c80%2c0%2c1%2c0%2c80%2c80H186.667A66.846%2c66.846%2c0%2c0%2c1%2c120%2c186.667Z' data-evernote-id='657' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-replay js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='160'%3e%3cpath d='M120%2c41.9v-20c0-5-4-8-8-4l-44%2c28a5.865%2c5.865%2c0%2c0%2c0-3.3%2c7.6A5.943%2c5.943%2c0%2c0%2c0%2c68%2c56.8l43%2c29c5%2c4%2c9%2c1%2c9-4v-20a60%2c60%2c0%2c1%2c1-60%2c60H40a80%2c80%2c0%2c1%2c0%2c80-79.9Z' data-evernote-id='658' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-play js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='161'%3e%3cpath d='M62.8%2c199.5c-1%2c0.8-2.4%2c0.6-3.3-0.4c-0.4-0.5-0.6-1.1-0.5-1.8V42.6c-0.2-1.3%2c0.7-2.4%2c1.9-2.6c0.7-0.1%2c1.3%2c0.1%2c1.9%2c0.4l154.7%2c77.7c2.1%2c1.1%2c2.1%2c2.8%2c0%2c3.8L62.8%2c199.5z' data-evernote-id='659' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-pause js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='162'%3e%3cpath d='M100%2c194.9c0.2%2c2.6-1.8%2c4.8-4.4%2c5c-0.2%2c0-0.4%2c0-0.6%2c0H65c-2.6%2c0.2-4.8-1.8-5-4.4c0-0.2%2c0-0.4%2c0-0.6V45c-0.2-2.6%2c1.8-4.8%2c4.4-5c0.2%2c0%2c0.4%2c0%2c0.6%2c0h30c2.6-0.2%2c4.8%2c1.8%2c5%2c4.4c0%2c0.2%2c0%2c0.4%2c0%2c0.6V194.9z M180%2c45.1c0.2-2.6-1.8-4.8-4.4-5c-0.2%2c0-0.4%2c0-0.6%2c0h-30c-2.6-0.2-4.8%2c1.8-5%2c4.4c0%2c0.2%2c0%2c0.4%2c0%2c0.6V195c-0.2%2c2.6%2c1.8%2c4.8%2c4.4%2c5c0.2%2c0%2c0.4%2c0%2c0.6%2c0h30c2.6%2c0.2%2c4.8-1.8%2c5-4.4c0-0.2%2c0-0.4%2c0-0.6V45.1z' data-evernote-id='660' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

Next Up
Futurist Warns of Dire Consequences from Corporate Control of AI
01:57

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-arrow-left js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='180'%3e%3cpath d='M55.4%2c104.4c-1.1%2c1.1-2.2%2c2.3-3.1%2c3.6c-6.9%2c9.9-4.4%2c23.5%2c5.5%2c30.4L159.7%2c240l33.9-33.9l-84.9-84.9l84.9-84.9L157.3%2c0L55.4%2c104.4L55.4%2c104.4z' data-evernote-id='756' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-play js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='181'%3e%3cpath d='M62.8%2c199.5c-1%2c0.8-2.4%2c0.6-3.3-0.4c-0.4-0.5-0.6-1.1-0.5-1.8V42.6c-0.2-1.3%2c0.7-2.4%2c1.9-2.6c0.7-0.1%2c1.3%2c0.1%2c1.9%2c0.4l154.7%2c77.7c2.1%2c1.1%2c2.1%2c2.8%2c0%2c3.8L62.8%2c199.5z' data-evernote-id='761' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

New AI technology able to 'read' ones facial impressions

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-play js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='182'%3e%3cpath d='M62.8%2c199.5c-1%2c0.8-2.4%2c0.6-3.3-0.4c-0.4-0.5-0.6-1.1-0.5-1.8V42.6c-0.2-1.3%2c0.7-2.4%2c1.9-2.6c0.7-0.1%2c1.3%2c0.1%2c1.9%2c0.4l154.7%2c77.7c2.1%2c1.1%2c2.1%2c2.8%2c0%2c3.8L62.8%2c199.5z' data-evernote-id='767' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

Futurist Warns of Dire Consequences from Corporate Control of AI

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-play js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='183'%3e%3cpath d='M62.8%2c199.5c-1%2c0.8-2.4%2c0.6-3.3-0.4c-0.4-0.5-0.6-1.1-0.5-1.8V42.6c-0.2-1.3%2c0.7-2.4%2c1.9-2.6c0.7-0.1%2c1.3%2c0.1%2c1.9%2c0.4l154.7%2c77.7c2.1%2c1.1%2c2.1%2c2.8%2c0%2c3.8L62.8%2c199.5z' data-evernote-id='773' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

SparkCognition CEO: Government Should Help Workers Who Lose Jobs to AI

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-play js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='184'%3e%3cpath d='M62.8%2c199.5c-1%2c0.8-2.4%2c0.6-3.3-0.4c-0.4-0.5-0.6-1.1-0.5-1.8V42.6c-0.2-1.3%2c0.7-2.4%2c1.9-2.6c0.7-0.1%2c1.3%2c0.1%2c1.9%2c0.4l154.7%2c77.7c2.1%2c1.1%2c2.1%2c2.8%2c0%2c3.8L62.8%2c199.5z' data-evernote-id='779' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

IBM Research Lead: Early Blood Test for Alzheimer's May Be Possible Through AI

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-play js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='185'%3e%3cpath d='M62.8%2c199.5c-1%2c0.8-2.4%2c0.6-3.3-0.4c-0.4-0.5-0.6-1.1-0.5-1.8V42.6c-0.2-1.3%2c0.7-2.4%2c1.9-2.6c0.7-0.1%2c1.3%2c0.1%2c1.9%2c0.4l154.7%2c77.7c2.1%2c1.1%2c2.1%2c2.8%2c0%2c3.8L62.8%2c199.5z' data-evernote-id='785' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

Bill Gates hopes AI improves medical education

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-play js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='186'%3e%3cpath d='M62.8%2c199.5c-1%2c0.8-2.4%2c0.6-3.3-0.4c-0.4-0.5-0.6-1.1-0.5-1.8V42.6c-0.2-1.3%2c0.7-2.4%2c1.9-2.6c0.7-0.1%2c1.3%2c0.1%2c1.9%2c0.4l154.7%2c77.7c2.1%2c1.1%2c2.1%2c2.8%2c0%2c3.8L62.8%2c199.5z' data-evernote-id='791' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

Blueprint York Fireside Chat: Impact of AI & automation on opportunities in the Heartland

![](data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' class='jw-svg-icon jw-svg-icon-arrow-right js-evernote-checked' viewBox='0 0 240 240' focusable='false' data-evernote-id='187'%3e%3cpath d='M183.6%2c104.4L81.8%2c0L45.4%2c36.3l84.9%2c84.9l-84.9%2c84.9L79.3%2c240l101.9-101.7c9.9-6.9%2c12.4-20.4%2c5.5-30.4C185.8%2c106.7%2c184.8%2c105.4%2c183.6%2c104.4L183.6%2c104.4z' data-evernote-id='796' class='js-evernote-checked'%3e%3c/path%3e%3c/svg%3e)

In November, Whittaker and others spearheaded the mass walkout of 20,000 Google employees to bring attention to what they characterized as a culture of complicity and dismissiveness. They pointed to Google’s policy of forced arbitration and a reported $90 million payout to Android founder and former Google executive Andy Rubin, who’s been accused of sexual misconduct, but also a Pentagon contract — Project Maven — that sought to implement object recognition in military drones. “[It’s] clear that we need real structural change, not adjustments to the status quo,” [said](https://medium.com/@GoogleWalkout/google-employees-and-contractors-participate-in-global-walkout-for-real-change-389c65517843) Whittaker.

## The problematic optics of Google’s AI ethics

Is this all coincidental? Perhaps. A Google spokesperson [told](https://www.nytimes.com/2019/04/22/technology/google-walkout-employees-retaliation.html) the *New York Times* that the company “prohibit[s] retaliation in the workplace … and investigate[s] all allegations,” and VentureBeat has reached out separately for clarification.

Regardless, Whittaker’s treatment sets an alarming precedent for a company that has in recent months struggled with ethics reviews. Just weeks ago, Google disbanded an external advisory board — the Advanced Technology External Advisory Council — that was tasked with ensuring its many divisions adhered to [seven guiding AI principles](https://www.blog.google/technology/ai/ai-principles/) set out last summer by CEO Sundar Pichai.

The eight-member panel was roundly criticized for its inclusion of Heritage Foundation president Kay Coles James, who has made negative remarks about trans people and whose organization is notably skeptical of climate change. And pundits like Vox’s [Kelsey Piper](https://www.vox.com/future-perfect/2019/4/4/18295933/google-cancels-ai-ethics-board) argued that the board, which would have convened only four times per year, lacked an avenue to evaluate — or even arrive at a clear understanding of — the AI work in which Google is involved.

Following the swift dissolution of Google’s external ethics board, the U.K.-based panel that offered counsel to DeepMind Health — the health care subsidiary of DeepMind, the AI firm Google acquired in 2014 — announced that it, too, would close, but for arguably more disconcerting reasons. Several of its members [told](https://www.wsj.com/articles/google-quietly-disbanded-another-ai-review-board-following-disagreements-11555250401)  *The Wall Street Journal* they hadn’t been afforded enough time or information to fulfill their questioning, and that they were concerned that Google and DeepMind’s close relationship posed a privacy risk.

Unsurprisingly, Google contends that its internal ethics boards quite successfully serve the role of watchdogs by regularly assessing new “projects, products, and deals.” It also points out that, in the past, it’s pledged not to commercialize certain technologies — chiefly general-purpose facial recognition — before lingering policy questions are addressed, and that it has both self-interrogated its AI initiatives and thoroughly detailed issues concerning AI governance, including explainability standards, fairness appraisal, safety consideration, and liability frameworks.

Setting aside for a moment recent allegations of retaliation, which Google denies, the company’s recent business decisions involving AI-driven products and research instill little confidence.

Reports emerged that this summer that Google contributed TensorFlow, its open source AI framework, while under a Pentagon contract — [Project Maven](https://venturebeat.com/2018/06/01/google-will-end-project-maven-military-contract-in-2019/) — that sought to implement object recognition in military drones. The company reportedly also planned to build a surveillance system that would have allowed Defense Department analysts and contractors to “click on” buildings, vehicles, people, large crowds, and landmarks and “see everything associated with [them].”

Other, smaller gaffes include failing to include both feminine and masculine translations for some languages in [Google Translate](https://venturebeat.com/2018/12/06/google-translate-now-returns-both-feminine-and-masculine-translations-for-words-and-phrases/), Google’s freely available language translation tool, and deploying a biased image classifier in Google Photos that mistakenly labeled a black couple as “gorillas.”

In early April during an [interview](https://www.recode.net/podcasts/2019/4/8/18299736/artificial-intelligence-ai-meredith-whittaker-kate-crawford-kara-swisher-decode-podcast-interview) with Recode’s Kara Swisher, Whittaker discussed unaudited, unregulated, and unsupervised AI’s potential impact. “You have systems that are determining which school your child gets enrolled in,” she said. “You have automated essay scoring systems that are determining whether it’s written well enough. Whose version of written English is that? And what is it rewarding or not? What kind of creativity can get through that?”

Whittaker wasn’t articulating a thought experiment; there are countless examples of AI gone awry. [Scientists claim](https://venturebeat.com/2019/01/24/amazon-rekognition-bias-mit/) that Amazon Web Services’ object detection API fails to reliably determine the sex of female and darker-skinned faces in specific scenarios. In February, researchers at the MIT Media Lab [found](https://www.media.mit.edu/articles/facial-recognition-software-is-biased-towards-white-men-researcher-finds/) that facial recognition made by Microsoft, IBM, and Chinese company Megvii misidentified gender in up to 7% of lighter-skinned females, up to 12% of darker-skinned males, and up to 35% of darker-skinned females. In a recent [study](https://venturebeat.com/2018/08/11/using-ai-and-big-data-to-address-the-accent-gap-in-voice-recognition-systems/) commissioned by the *Washington Post*, popular smart speakers made by Google and Amazon were 30% less likely to understand non-American accents than those of native-born users.

## Protesters, objectors, and whistleblowers

Google is by no means alone when it comes to AI ethics stumbles. A [cohort of AI researchers](https://venturebeat.com/2019/04/03/prominent-ai-researchers-call-on-amazon-to-stop-selling-rekognition-facial-analysis-to-law-enforcement/) recently called on Amazon’s AWS to stop selling Rekognition, a facial recognition service that’s been criticized for its binary classification of sexual orientation and uneven treatment of people of color, to law enforcement agencies. It’s a challenge the entire industry is struggling with, but often that struggle manifests in active protests and organized, public objections. In other cases it involves whistleblowers like Whittaker.

In an [editorial](https://www.nytimes.com/2019/04/23/opinion/google-privacy-china.html) published Monday in the *New York Times*, former Google research scientist and Stanford mathematics professor Jack Poulson highlighted employee revolts that led to the reversal of questionable projects pursued by tech giants, including Google’s air gap technology for the Air Force and its abandoned bid for a $10 billion Pentagon cloud computing contract. “Complaints from a single rank-and-file engineer aren’t going to lead a company to act against its significant financial interests,” wrote Poulson. “But history shows that dissenters — aided by courts or the court of public opinion — can sometimes make a difference. Even if that difference is just alerting the public to what these companies are up to.”

Meanwhile, a [study](http://www.europarl.europa.eu/stoa/en/document/EPRS_STU(2019)624262) published by the European Parliamentary Research Service, the in-house research department and think tank of the European Parliament, asserts that whistleblowers “play an important role” in uncovering “questionable uses and outcomes” of algorithmic decision-making. It specifically cites a *New York Times*  [investigation](https://www.nytimes.com/2017/03/03/technology/uber-greyball-program-evade-authorities.html) into an Uber algorithm that enabled the ride-hailing company to evade regulators, information that was supplied by current and former Uber employees; and a [ProPublica profile](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) of Compas, an automated recidivism assessment software that associated African American defendants with higher risk scores.

Guillaume Chaslot, a 36-year-old former Google employee who’s loudly criticized the tech industry’s treatment of ethics, is another example. He [explained](https://www.theguardian.com/technology/2018/feb/02/how-youtubes-algorithm-distorts-truth) to *The Guardian* how YouTube, which is owned by Google, is algorithmically engineered to increase advertising revenues with attention-retaining formulas. (For its part, Google says that Chaslot was let go for “performance issues” in 2013, and says that YouTube’s recommendation system now discourages the promotion of potentially inflammatory religious or supremacist content and takes into account things like “user satisfaction” and the number of “likes” a video has received.)

“Technology companies should provide protections for conscientious objectors, employee organizing, and ethical whistleblowers,” wrote Whittaker and colleagues in a [separate](https://ainowinstitute.org/AI_Now_2018_Report.pdf) AI Institute report last year. “The recent surge in activism has largely been driven by whistleblowers within technology companies, who have disclosed information about secretive projects to journalists. These disclosures have helped educate the public, which is traditionally excluded from such access, and helped external researchers and advocates provide more informed analysis.”

A recent whitepaper from Google concludes: “We support the collaborative and consultative process that many are pursuing, and encourage stakeholders everywhere to participate … [and we] hope to find opportunities for Google to continue to listen to, learn from, and contribute more actively to the wider discussion about AI’s impact on society.” That’s a call for respectful dialogue, but on its face, there seems to be little of that in the Whittaker case. If she was indeed pressured to step away from the AI Now Institute and demoted at Google as backlash for her ethics and advocacy work, that’s about silencing a voice, not conversing with it.

Whittaker isn’t a whistleblower in the strictest sense — she’s been transparent and forthright about her concerns with Google leadership, regulators, and members of the press. But even if her treatment doesn’t reach the threshold of retaliation, it sends a worrying message: Protests — particularly coordinated, attention-grabbing walkouts about controversial AI programs and workplace discrimination — aren’t welcome.