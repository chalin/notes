Millions of black people affected by racial bias in health-care algorithms

NEWS
·  24 October 2019

·

- Update [26 October 2019](https://www.nature.com/articles/d41586-019-03228-6#correction-0)

# Millions of black people affected by racial bias in health-care algorithms

Study reveals rampant racism in decision-making software used by US hospitals — and highlights ways to correct it.

### Show author information[Heidi Ledford](#)

- [![](data:image/svg+xml,%3csvg viewBox='0 0 30 30' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' data-evernote-id='185' class='js-evernote-checked'%3e %3ctitle%3eShare on Twitter%3c/title%3e %3cdesc%3eShare on Twitter%3c/desc%3e %3cdefs%3e%3c/defs%3e %3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd'%3e %3cg%3e %3cpolygon points='0 0 30 0 30 30 0 30'%3e%3c/polygon%3e %3cpath d='M20.8125%2c11.4875 C21.42%2c11.10375 21.8875%2c10.49625 22.105%2c9.7725 C21.5375%2c10.1275 20.90875%2c10.385 20.23875%2c10.5225 C19.70625%2c9.9225 18.9425%2c9.545 18.0975%2c9.545 C16.475%2c9.545 15.16%2c10.9325 15.16%2c12.6425 C15.16%2c12.885 15.185%2c13.1225 15.235%2c13.3475 C12.7975%2c13.2175 10.63125%2c11.985 9.1825%2c10.11 C8.93%2c10.56875 8.785%2c11.10125 8.785%2c11.66875 C8.785%2c12.74375 9.30375%2c13.69125 10.09125%2c14.2475 C9.61125%2c14.23125 9.1575%2c14.09 8.76125%2c13.86 L8.76125%2c13.8975 C8.76125%2c15.3975 9.77375%2c16.65125 11.11875%2c16.935 C10.87125%2c17.0075 10.6125%2c17.04375 10.34375%2c17.04375 C10.15625%2c17.04375 9.96875%2c17.025 9.79125%2c16.98875 C10.16625%2c18.22125 11.24875%2c19.11875 12.535%2c19.1425 C11.52875%2c19.97375 10.2625%2c20.4675 8.885%2c20.4675 C8.6475%2c20.4675 8.415%2c20.455 8.185%2c20.42625 C9.485%2c21.30375 11.02875%2c21.81625 12.6875%2c21.81625 C18.09%2c21.81625 21.04375%2c17.095 21.04375%2c13.00125 L21.03625%2c12.60125 C21.61125%2c12.16375 22.11125%2c11.6175 22.50125%2c10.99625 C21.97375%2c11.2425 21.4075%2c11.40875 20.81375%2c11.48375 L20.8125%2c11.4875 Z' fill-rule='nonzero' data-evernote-id='679' class='js-evernote-checked'%3e%3c/path%3e %3c/g%3e %3c/g%3e %3c/svg%3e)](https://twitter.com/intent/tweet?text=Millions+of+black+people+affected+by+racial+bias+in+health-care+algorithms&url=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-019-03228-6)

- [![](data:image/svg+xml,%3csvg viewBox='0 0 30 30' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' data-evernote-id='186' class='js-evernote-checked'%3e %3ctitle%3eShare on Facebook%3c/title%3e %3cdesc%3eShare on Facebook%3c/desc%3e %3cdefs%3e%3c/defs%3e %3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd'%3e %3cg%3e %3cpolygon points='0 0 30 0 30 30 0 30'%3e%3c/polygon%3e %3cpath d='M15.89625%2c22.8625 L12.57125%2c22.8625 L12.57125%2c15.02125 L10.90875%2c15.02125 L10.90875%2c12.31875 L12.57125%2c12.31875 L12.57125%2c10.69625 C12.57125%2c8.4925 13.50875%2c7.18 16.175%2c7.18 L18.39375%2c7.18 L18.39375%2c9.8825 L17.00625%2c9.8825 C15.96875%2c9.8825 15.9%2c10.26 15.9%2c10.965 L15.895%2c12.3175 L18.4075%2c12.3175 L18.115%2c15.02 L15.89625%2c15.02 L15.89625%2c22.8625 Z' fill-rule='nonzero' data-evernote-id='680' class='js-evernote-checked'%3e%3c/path%3e %3c/g%3e %3c/g%3e %3c/svg%3e)](http://www.facebook.com/sharer.php?u=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-019-03228-6)

- [![](data:image/svg+xml,%3csvg viewBox='0 0 30 30' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' data-evernote-id='187' class='js-evernote-checked'%3e %3ctitle%3eShare via E-Mail%3c/title%3e %3cdesc%3eShare via E-Mail%3c/desc%3e %3cdefs%3e%3c/defs%3e %3cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd'%3e %3cg%3e %3cg%3e %3cpolygon points='0 0 30 0 30 30 0 30'%3e%3c/polygon%3e %3cpath d='M15%2c15.3269887 L10.6248577%2c11.9177869 C10.4236021%2c11.7609644 10.1299323%2c11.7927468 9.96892789%2c11.988775 C9.80792343%2c12.1848031 9.84055341%2c12.4708451 10.041809%2c12.6276676 L14.7012493%2c16.2584003 C14.8680779%2c16.3940555 15.1152493%2c16.4013884 15.2915244%2c16.2640313 C15.2939898%2c16.2622325 15.2963784%2c16.2603294 15.2987507%2c16.2584003 L19.958191%2c12.6276676 C20.1594466%2c12.4708451 20.1920766%2c12.1848031 20.0310721%2c11.988775 C19.8700677%2c11.7927468 19.5763979%2c11.7609644 19.3751423%2c11.9177869 L15%2c15.3269887 Z M9%2c10 L21%2c10 C21.5522847%2c10 22%2c10.4477153 22%2c11 L22%2c19 C22%2c19.5522847 21.5522847%2c20 21%2c20 L9%2c20 C8.44771525%2c20 8%2c19.5522847 8%2c19 L8%2c11 C8%2c10.4477153 8.44771525%2c10 9%2c10 Z' data-evernote-id='681' class='js-evernote-checked'%3e%3c/path%3e %3c/g%3e %3c/g%3e %3c/g%3e %3c/svg%3e)](https://www.nature.com/articles/d41586-019-03228-6mailto:?subject=Millions%20of%20black%20people%20affected%20by%20racial%20bias%20in%20health-care%20algorithms&body=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-019-03228-6)

 ![d41586-019-03228-6_17304098.jpg](../_resources/813f3db16bad03f75fc436e22be3a256.jpg)

Black people with complex medical needs were less likely than equally ill white people to be referred to programmes that provide more personalized care.Credit: Ed Kashi/VII/Redux/eyevine

An algorithm widely used in US hospitals to allocate health care to patients has been systematically discriminating against black people, a sweeping analysis has found.

The study, published in *Science* on 24 October[1](https://www.nature.com/articles/d41586-019-03228-6#ref-CR1), concluded that the algorithm was less likely to refer black people than white people who were equally sick to programmes that aim to improve care for patients with complex medical needs. Hospitals and insurers use the algorithm and others like it to help manage care for about 200 million people in the United States each year.

This type of study is rare, because researchers often cannot gain access to proprietary algorithms and the reams of sensitive health data needed to fully test them, says Milena Gianfrancesco, an epidemiologist at the University of California, San Francisco, who has studied sources of bias in electronic medical records. But smaller studies and anecdotal reports have documented unfair and [biased decision-making by algorithms](https://www.nature.com/articles/d41586-018-05469-3) used in everything from criminal justice to education and health care.

“It is alarming,” says Gianfrancesco of the latest study. “We need a better way of actually assessing the health of the patients.”

Ziad Obermeyer, who studies machine learning and health-care management at the University of California, Berkeley, and his team stumbled onto the problem while examining the impact of programmes that provide additional resources and closer medical supervision for people with multiple, sometimes overlapping, health problems.

## Examining assumptions

When Obermeyer and his colleagues ran routine statistical checks on data they received from a large hospital, they were surprised to find that people who self-identified as black were generally assigned lower risk scores than equally sick white people. As a result, the black people were less likely to be referred to the programmes that provide more-personalized care.

The researchers found that the algorithm assigned risk scores to patients on the basis of total health-care costs accrued in one year. They say that this assumption might have seemed reasonable because higher health-care costs are generally associated with greater health needs. The average black person in the data set that the scientists used had similar overall health-care costs to the average white person.

But a closer look at the data revealed that the average black person was also substantially sicker than the average white person, with a greater prevalence of conditions such as diabetes, anaemia, kidney failure and high blood pressure. Taken together, the data showed that the care provided to black people cost an average of US$1,800 less per year than the care given to a white person with the same number of chronic health problems.

The scientists speculate that this reduced access to care is due to the effects of systemic racism, ranging from distrust of the health-care system to direct racial discrimination by health-care providers.

And because the algorithm assigned people to high-risk categories on the basis of costs, those biases were passed on in its results: black people had to be sicker than white people before being referred for additional help. Only 17.7% of patients that the algorithm assigned to receive extra care were black. The researchers calculate that the proportion would be 46.5% if the algorithm were unbiased.

## Seeking solutions

When Obermeyer and his team reported their findings to the algorithm’s developers — Optum of Eden Prairie, Minnesota — the company repeated their analysis and found the same results. Obermeyer is working with the firm without salary to improve the algorithm.

He and his team collaborated with the company to find variables other than healthcare costs that could be used to calculate a person's medical needs, and repeated their analysis after tweaking the algorithm accordingly. They found that making these changes reduced bias by 84%.

“We appreciate the researchers’ work,” Optum said in a statement. But the company added that it considered the researchers' conclusion to be “misleading”. “The cost model is just one of many data elements intended to be used to select patients for clinical engagement programs, including, most importantly, the doctor's expertise.”

Obermeyer says that using cost prediction to make decisions about patient engagement is a pervasive issue. “This is not a problem with one algorithm, or one company — it’s a problem with how our entire system approaches this problem,” he says.

Finding fixes for bias in algorithms — in health care and beyond — is not straightforward, Obermeyer says. “Those solutions are easy in a software engineering sense: you just rerun the algorithm with another variable,” he says. “But the hard part is: what is that other variable? How do you work around the bias and injustice that is inherent in that society?”

This is in part because of a lack of diversity among algorithm designers, and a lack of training about the social and historical context of their work, says Ruha Benjamin, author of *Race After Technology* (2019) and a sociologist at Princeton University in New Jersey.

“We can’t rely on the people who currently design these systems to fully anticipate or mitigate all the harms associated with automation,” she says.

Developers should run tests such as those performed by Obermeyer’s group routinely before deploying an algorithm that affects human lives, says Rayid Ghani, a computer scientist at Carnegie Mellon University in Pittsburgh, Pennsylvania. That kind of auditing is more common now, he says, since reports of biased algorithms have become more widespread.

“Are more doing it now than used to? Yes,” says Ghani. “Are enough of them doing it? No.”

He thinks that the results of these audits should always be compared to human decision making before assuming that an algorithm is making things worse. Ghani says that his team has carried out unpublished analyses comparing algorithms used in public health, criminal justice and education to human decision making. They found that the machine-learning systems were biased — but less so than the people.

“We are still using these algorithms called humans that are really biased,” says Ghani. “We’ve tested them and known that they’re horrible, but we still use them to make really important decisions every day.”

*Nature*  **574**, 608-609 (2019)

doi: 10.1038/d41586-019-03228-6

### Updates & Corrections

- **Update 26 October 2019**: Added the name of the algorithm developer and the company’s response to the study, as well as additional comments from Ziad Obermeyer.

## References

1. 1.

Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. *Science*  **336**, 447–453 (2019).

    -

        - [Google Scholar](http://scholar.google.com/scholar_lookup?&journal=Science%20%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B&volume=336&pages=447-453&publication_year=2019&author=Obermeyer%2CZ.&author=Powers%2CB.&author=Vogeli%2CC.&author=Mullainathan%2CS.)

[Download references](https://www.nature.com/articles/d41586-019-03228-6-references.ris)

Latest on:

Computer science

Health care

Policy

 [ ### Challenge to test reproducibility of old computer code    Correspondence|  29 OCT 19](http://www.nature.com/articles/d41586-019-03296-8)

 [![d41586-019-03173-4_17295604.png](../_resources/b229cdf96659d6da0ad467fd756a715b.png)   ### Quantum computing takes flight    News & Views|  23 OCT 19](http://www.nature.com/articles/d41586-019-03173-4)

 [![d41586-019-03213-z_17297208.jpg](../_resources/0c4f845de1e912363aa2bc3fc3edb277.jpg)   ### Hello quantum world! Google publishes landmark quantum supremacy claim    News|  23 OCT 19](http://www.nature.com/articles/d41586-019-03213-z)

# Nature Briefing

An essential round-up of science news, opinion and analysis, delivered to your inbox every weekday.

Email address

Yes! Sign me up to receive the daily *Nature Briefing* email. I agree my information will be processed in accordance with the *Nature* and Springer Nature Limited [Privacy Policy](https://www.nature.com/info/privacy).

### Related Articles

-

###   [![d41586-019-03228-6_17301856.gif](../_resources/8c9ccd611a760773d77d0493c898e4d2.gif) A fairer way forward for AI in health care](https://www.nature.com/articles/d41586-019-02872-2)

-

###   [![d41586-019-03228-6_16216744.jpg](../_resources/a04bfdcfce00ffa689f12da93deda3e6.jpg) Bias detectives: the researchers striving to make algorithms fair](https://www.nature.com/articles/d41586-018-05469-3)

-

###   [Can we open the black box of AI?](https://www.nature.com/news/can-we-open-the-black-box-of-ai-1.20731)

### Subjects

- [Computer science](https://www.nature.com/subjects/computer-science)

- [Health care](https://www.nature.com/subjects/health-care)

- [Policy](https://www.nature.com/subjects/policy)

- [Society](https://www.nature.com/subjects/society)