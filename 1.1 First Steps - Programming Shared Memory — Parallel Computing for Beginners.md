1.1 First Steps - Programming Shared Memory — Parallel Computing for Beginners

# 1.1 First Steps - Programming Shared Memory[¶](https://pdcbook.calvin.edu/pdcbook/PDCBeginners/sharedMemory/firststeps.html#first-steps-programming-shared-memory)

In this book, we use the Open Multi-Processing (**OpenMP**) library to demonstrate how to program shared memory systems. While OpenMP is just one of many options for programming shared memory systems, we choose OpenMP due to its pervasiveness and relative ease of use. OpenMP employs a series of compiler directives to enable users to incrementally add parallelism to their programs and is natively supported by GCC.

## 1.1.1 The Fork-Join Pattern[¶](https://pdcbook.calvin.edu/pdcbook/PDCBeginners/sharedMemory/firststeps.html#the-fork-join-pattern)

As a means of introducing OpenMP, we begin by introducing the **fork-join pattern** for parallel programming.

![ForkJoin.png](../_resources/96ea0587e3b13b5a8b3328a9e5a1b1e8.png)

In fork-join, the main thread “forks” (or creates) a series of threads that each perform a separate task in parallel. When each thread terminates, their executions “join” (or merge) together back into the main thread.

Consider the following serial program. What is its output?

[(L)](https://pdcbook.calvin.edu/pdcbook/PDCBeginners/sharedMemory/firststeps.html#)

Original - 1 of 1

xxxxxxxxxx

1
#include <stdio.h> // printf()
2
#include <omp.h> // OpenMP
3
​
4
int main(int argc, char** argv) {
5
​
6
printf("\nBefore...\n");
7
​
8
// #pragma omp parallel
9
printf("\nDuring...");
10
​
11
printf("\n\nAfter...\n\n");
12
​
13
return 0;
14
}
15
​

Before...

During...

After...

Activity: 1 -- Serial Fork-Join (sm_fork_join)

The above code simply prints out the strings `Before`, `During` and `After` in order.

Now *uncomment* the `omp parallel pragma` on line 8 and re-run the program.

Q-1: What happens when you re-run the example?

A. Nothing happens. It's the same output.

B. The three strings ``Before``, ``During`` and ``After`` are printed multiple times.

C. The string ``During`` is printed multiple times.
D. The strings ``During`` and ``After`` are each printed multiple times.

Activity: 2 -- Multiple Choice (sm_mc_fork_1)

The `omp parallel pragma` on line 8, when uncommented, tells the compiler to fork a set of threads to execute the next *line* of code (later you will see how this is done for a block of code). Next, the `omp parallel pragma` creates a team of threads and directs each thread to run the line `printf(\nDuring...)` in parallel.

Thus, the string `During` is printed out a number of times that correspond to the number cores on the system (in this case, 4). Note that in OpenMP the join is implicit and does not require a pragma directive.

## 1.1.2 The SPMD Pattern[¶](https://pdcbook.calvin.edu/pdcbook/PDCBeginners/sharedMemory/firststeps.html#the-spmd-pattern)

A common use of the fork-join pattern is to have each thread run the same block of code on different components of data. This pattern is known as**single program multiple data** or the **SPMD** pattern. Let’s try running a new code snippet:

xxxxxxxxxx

1
#include <stdio.h>
2
#include <omp.h>
3
​
4
int main(int argc, char** argv) {
5
printf("\n");
6
​
7
// #pragma omp parallel
8
{
9
int id = omp_get_thread_num();
10
int numThreads = omp_get_num_threads();
11
printf("Hello from thread %d of %d\n", id, numThreads);
12
}
13
​
14
printf("\n");
15
return 0;
16
}
17
​

Activity: 3 -- SPMD (serial) (sm_spmd_serial)

When the `omp parallel` pragma executes, it assigns each thread a unique id (from `0` to `n-1` for *n* threads). A programmer can access this unique id by calling the `omp_get_thread_num()` function. Likewise, OpenMP provides the`omp_get_num_threads()` function to provide the programmer the ability to see the total number of threads. On a single threaded program (like the one shown above), there is 1 total thread, with a thread id of `0`.

Consider what will happen when the pragma above is uncommented (recall that there are `4` total cores on the system). What do you think the output would be?

Q-2: What will be the output when the pragma is uncommented in the spmd_serial program?

A. There will be 4 hello messages, each having the thread id 0

B. There will be 4 hello messages, each having different thread ids, printed in order

C. There will be 4 hello messages, each having different thread ids, printed in random order

D. Something else

Activity: 4 -- Multiple Choice (sm_mc_spmd_1)

Let’s now run a version of the program with the `omp parallel` pragma uncommented:

xxxxxxxxxx

1
#include <stdio.h>
2
#include <omp.h>
3
​
4
int main(int argc, char** argv) {
5
printf("\n");
6
​
7
#pragma omp parallel //this line is now uncommented!
8
{
9
int id = omp_get_thread_num();
10
int numThreads = omp_get_num_threads();
11
printf("Hello from thread %d of %d\n", id, numThreads);
12
}
13
​
14
printf("\n");
15
return 0;
16
}
17
​

Activity: 5 -- SPMD (parallel) (sm_spmd_parallel)

Running this program reveals two things. First, since there are 4 total cores on the system, the `omp parallel`pragma generates a team of 4 threads, assigning each a unique id from 0 to 3. Each thread then runs the code in the scope of the pragma (denoted by curly braces). The process can be visualized as follows:

![ForkJoin_SPMD.png](../_resources/3f3fd06ee7707e6a812cbe76a6ef4976.png)

The code in main up until line 6 is run in one thread on one core; the forking of separate threads to run the code between lines 7 and 12 is shown in the middle of the diagram. The final last couple of lines of code are run back in the single thread 0 after all the threads have completed and join back to the main thread.

Q-3: Try re-running the sm_spmd_parallel example a few times. What do you observe about the order of the printed lines?

A. The hello messages always print in order (0 .. 3)
B. The ordering of the hello messages is random and cannot be predicted.

C. The hello messages always prints in a random order, but is consistent over multiple runs

Activity: 6 -- Multiple Choice (sm_mc_spmd_2)

Re-running the program multiple times illustrates an important point about threaded programs: the ordering of execution of statements between threads is *not* guaranteed. In fact, the order in which any set of threads execute is not guaranteed, and is determined by the operating system. This situation illustrates the concept of **non-determinism**, where an algorithm or program can have different outputs over multiple runs. While all parallel algorithms have inherent non-deterministic properties, experienced programmers can *leverage* the non-deterministic execution to their advantage (e.g. run the code on multiple cores) and still get correct output. We will study several such examples in the coming sections.

## 1.1.3 A Larger Example - Filling an Array[¶](https://pdcbook.calvin.edu/pdcbook/PDCBeginners/sharedMemory/firststeps.html#a-larger-example-filling-an-array)

The fork-join and SPMD patterns are some of the most widely used for programming shared memory systems. In general, the fork-join pattern is used for **task parallelism**, or when a team of threads receive a component of a larger problem and work together to come up with a solution. The SPMD pattern is commonly used for **data parallelism** where a team of threads run the same program on different components of*data* or *memory*. In this scenario, each thread does the exact same task – the only difference is that each thread is operating on a different unit of data or memory.

As an example, consider the process of filling an array of size *n* with elements from 1 .. *n* -1:

The following snippet of C code is a serial implementation that populates an array with 20 million elements:

xxxxxxxxxx

1
#include <stdio.h>
2
#include <stdlib.h>
3
​
4
#define N 20000000 //size of the array
5
​
6
int main(void){
7
​
8
int * array = malloc(N*sizeof(int)); //declare array of size N
9
int i;
10
​
11
//populate array
12
for (i = 0; i < N; i++) {
13
array[i] = i+1;
14
}
15
​
16
printf("Done populating %d elements!", N);
17
return 0;
18
}
19
​

Activity: 7 -- Array Fill (serial) (sm_arrayfill_serial)

Let’s consider how we would parallelize a program like this. One way is to assign each thread a different segment of the array, and have each thread populate its own component of the array. The following video illustrates how 4 threads would populate an array (each thread is assigned a different color):

Q-4: Is populating an array in parallel an example of data parallelism or task parallelism?

A. Task parallelism
B. Data parallelism
C. Neither

Activity: 8 -- Multiple Choice (sm_mc_tpdp_1)

The notions of “task parallelism” and “data parallelism” are two extremes on a spectrum. Most parallel programs fall somewhere along the spectrum. For now, it is sufficient to recognize that both fork-join and SPMD are valid ways to assign work to threads.

## 1.1.3 For-Loop Pragmas[¶](https://pdcbook.calvin.edu/pdcbook/PDCBeginners/sharedMemory/firststeps.html#for-loop-pragmas)

Before we parallelize the populate array program, we need to introduce two new pragmas. The first is the`omp for` pragma. This pragma parallelizes the iterations of a for loop by assigning each thread a chunk of iterations of the loop. The following code snippet illustrates how to use the `omp for` pragma to parallelize the populate array program:

xxxxxxxxxx

1
#include <stdio.h>
2
#include <stdlib.h>
3
​
4
#define N 20000000 //size of the array
5
​
6
int main(void){
7
​
8
int * array = malloc(N*sizeof(int)); //declare array of size N
9
​
10
#pragma omp parallel //<-- entered omp parallel pragma here
11
{
12
//populate array
13
​
14
#pragma omp for //<-- entered omp for pragma here
15
for (int i = 0; i < N; i++) {
16
array[i] = i+1;
17
}
18
​
19
}
20
printf("Done populating %d elements!", N);
21
return 0;
22
}
23
​

Activity: 9 -- Array Fill (serial) (sm_arrayfill_parallel1)

Notice that in the body of the above program that there is nothing between the `omp parallel` and the `omp for` pragmas. This is fairly common, as sometimes the key piece of code to be parallelized is just a for loop. To simplify the process for programmers, OpenMP provides the `omp parallel for` pragma, which literally combines the functionality of the `omp parallel` and the `omp for` pragmas into one line of code.

The following program illustrates this new pragma in action:

xxxxxxxxxx

1
#include <stdio.h>
2
#include <stdlib.h>
3
​
4
#define N 20000000 //size of the array
5
​
6
int main(void){
7
​
8
int * array = malloc(N*sizeof(int)); //declare array of size N
9
int i;
10
​
11
//populate array
12
#pragma omp parallel for //<-- inserted omp parallel for pragma
13
for (i = 0; i < N; i++) {
14
array[i] = i+1;
15
}
16
​
17
printf("Done populating %d elements!", N);
18
return 0;
19
}
20
​

Activity: 10 -- Array Fill (serial) (sm_arrayfill_parallel_for)

Notice how much shorter and simpler this code is. However, the `omp parallel for` isn’t always appropriate for all cases, as we will see in the next section.